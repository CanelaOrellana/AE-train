# Carpeta de C√≥digos

Aqu√≠ se encuentra:
* el c√≥digo `AutoEncoders.py`
* el c√≥digo `CasoBase_train.ipynb`
* la carpeta **Detecci√≥n de Anomal√≠as**

_ _ _
_ _ _
## ‚ú®AutoEncoders.py‚ú®

En este script de python, se encuentran las clases que definen a los objetos AutoEncoder y ClassifierAutoEncoder. A continuaci√≥n les explico qu√© se puede hacer con cada una de ellas, en qu√© consisten los objetos, qu√©e m√©todos traen y cuales son los hiperpar√°metros que pueden utilizar para obtener el autoencoder de sus sue√±os. 

_pd: obvio no olviden que cuando clonen este repo o copien los c√≥digos, podr√°n agregar y quitar los m√©todos que quieran_

### Clase ‚ú®<span style="color:teal">AutoEncoder</span>‚ú®


Posee los hiperpar√°metros:
- `h_layer_sizes` <span style="color:green">List [ int ]</span> : recibe una lista, la cual contiene los tama√±os de las capas ocultas del encoder (y por ende, del decoder)  

- `timesteps` <span style="color:green">int</span> : representa la cantidad de pasos en el tiempo que se reciben, es el tama√±o de la ventana temporal. (Si se trabajara con frases, ser√≠a el n√∫mero de palabras)  

- `learning_rate` <span style="color:green">float</span> :  corresponde al learning rate de toda la vida  

- `dropout` <span style="color:green">float</span> : corresponde al dropout que se aplica luego de la capa de salida del encoder y del decoder.  
_pd: nunca obtuve mejor rendimiento utiliz√°ndolo_  

- `n_features_in` <span style="color:green">int</span> : es el n√∫mero de caracter√≠sticas de entrada de los datos que se est√°n ingresando, en mi caso, correspond√≠a a la cantidad de sensores que estaba considerando en mis datos de entrenamiento  

- `batch_size` <span style="color:green">int</span> : es el batch_size de toda la vida.  
_pd: en mi experiencia es mejor utiliza n√∫meros peque√±os (menores a 10), no s√≥lo por eel desempe√±o obtenido, sino que tambi√©n por la capacidad de la gpu que yo utilic√©_  

- `optim` <span style="color:green">string</span>:  puede ser <span style="color:orange">'Adam'</span>, <span style="color:orange">'RAdam'</span> o <span style="color:orange">'AdamW'</span> y permite elegir qu√© optimizador desea usar en el entrenamiento futuro.  
_pd: s√≥lo se configur√≥ para esas 3 posibilidades, si se desea utilizar otro optimizador, se debe modificar un poco el c√≥digo.  
pd 2: siempre me funcion√≥ mejor RAdam (tiene sentido)_  

- `epochs` <span style="color:green">int</span> : representa el n√∫mero de √©pocas (m√°ximo) durante las que se llevar√° a cabo el entrenamiento.  

- `verbose` <span style="color:green">int</span> : permite controlar cu√°ntos mensajes informativos se quiere recibir durante el entrenamiento. Como regla general, se reciben mensajes en las √©pocas m√∫ltiplos del n√∫mero que se utilice, el mensaje incluye: √©poca, loss en validaci√≥n, R^2 en validaci√≥n y R^2 en entrenamiento.  
Tambi√©n hay algunos n√∫meros especiales:
    - **0**: Se imprime el n√∫mero de capas, leearning rate, dropout y paciencia que se est√°n utilizando en el entrenamiento (No se imprime nada durante los bucles de entrenamiento).  
    _pd: el objetivo de esto era seguirle la pista a los entrenamientos al utilizar GridSearch_  
    - **5**: Adem√°s de los mensajes cada 5 √©pocas, al finalizar el entrenamiento se muestran los gr√°ficos de **loss vs √©pocas** y de **r^2 vs √©pocas**. 

- `patience` <span style="color:green">int</span> : representa la cantidad de √©pocas que se tendr√° paciencia si el modelo no est√° bajando su funci√≥n de _loss_, en caso de que se superen las √©pocas de paciencia, se detendr√° tempranamente el entrenamiento del modelo, ya que no se est√°n obteniendo mejoras y no queremos malgastar ni tiempo ni recursos de computo.

M√©todos que posee:

- `build_model`: permite construir la arquitectura del modelo seg√∫n los hiperpar√°metros antes definidos. Posee 3 clases locales
    - Encoder: permite crear un objeto Encoder a partir de los hiperpar√°metros antes definidos, incluyendo su m√©todo fordward.    
    - Decoder: permite crear un objeto Decoder a partir de los hiperpar√°metros antes definidos, incluyendo su m√©todo fordward.  
    - Autoencoder: permite crear al objeto Autoencoder, que consiste en un Encoder seguido de un Decoder, incluyendo su m√©todo fordward.  
En este m√©todo retorna el modelo de arquitectura autoencoder, tambi√©n se definen aqu√≠, la funci√≥n de loss y el optimizador del modelo.  

-  `fit`: este m√©todo permite llevar a cabo los bucles de entrenamiento del modelo, utilizando de apoyo a las funciones `ftrain` y `fevaluate`. Para funcionar recibe:
    - **X**: conjunto de datos de entrenamiento, deben estar en el formato esperado []
    - `p_train` <span style="color:green">float</span> : representa el porcentaje de X que se usar√° para entrenamiento, el porcentaje restante se utilizar√° para validaci√≥n.  
    _pd: estos AEs est√°n dise√±ados para el procesamiento de se√±ales, por lo que no se realiza shuffle de los conjuntos. Tenerlo presente si se desea utilizar estos c√≥digos con otro tipo de datos_  

- `predict`: este m√©todo permite predecir utilizando el modelo entrenado (o no).

- `score`: este m√©todo permite calcular una m√©trica para evaluar la predicci√≥n del modelo. En el c√≥digo se puede modificar para utilizar mse o r^2.  
_pd: este m√©todo es utilizado por GridSearch para definir qu√© modelos de los entrenados son mejores_

- `save_weights`: este m√©todo permite guardar los pesos del modelo entrenado, para en un futuro poder simplemente cargarlos en el modelo, sin necesidad de reentrenar.  

- `transform`: este m√©todo permite transformar los datos a la forma del espacio latente del autoencoder.  
_pd: existe por completitud de los m√©todos que se implementan en los modelos de scikit learn, que es para los que funciona GridSearch de manera nativa_  
_pd 2: pero por su puesto, el espacio latente que se produce es de gran inter√©s üíó_

- `inverse_transform`: este m√©todo permite transformar los datos desde el espacio latente al espacio de entrada al autoencoder.  
_pd: tambi√©n existe por completitud de los m√©todos que se implementan en los modelos de scikit learn, que es para los que funciona GridSearch de manera nativa_

#### Clase ‚ú®AutoEncoderWrapper‚ú®
Esta clase sirve para _envolver_ nuestro autoencoder creado con la clase explicada anteriormente. Por lo que recibe exactamente los mismos hiperpar√°metros que <span style="color:teal">AutoEncoder</span>. Sus m√©todos se describen a continuaci√≥n

- `_create_autoencoder`: este m√©todo retorna un objeto de la clase AutoEncoder, con los hiperpar√°metros recibidos.
- `fit`: este m√©todo utiliza el m√©todo anterior para crear un autoencoder y lo entrena llamando al m√©todo <span style="color:orange">fit</span> de AutoEncoder.
- `predict`: este m√©todo llama al m√©todo <span style="color:orange">predict</span> de AutoEncoder.
- `score`: este m√©todo llama al m√©todo <span style="color:orange">score</span> de AutoEncoder.
- `save_weights`: este m√©todo llama al m√©todo <span style="color:orange">save\_weights</span> de AutoEncoder.

Este objeto permite utilizar los m√©todos de `GridSearch` o `HalvingGridSearch` para encontrar los mejores hiperpar√°metros


### Clase ‚ú®<span style="color:teal">ClassifierAutoEncoder</span>‚ú®

P R O X I M A M E N T E

_ _ _
_ _ _
## ‚ú®CasoBase_train.ipynb‚ú®

En este notebook se muestran ejemplos para
- Realizar los entrenamientos utilizando grillas
- Realizar un entrenamiento espec√≠fico
- Cargar los pesos para un uso posterior del modelo




_ _ _
_ _ _
## ‚ú®Detecci√≥n de Anomal√≠as‚ú®

En esta carpeta a√∫n no pueden encontrar nada, pero pr√≥ximamente estar√°n los c√≥digos de c√≥mo se puede realizar detecci√≥n de anomal√≠as a partir de los autoencoders creados 